{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import math\n",
    "import operator\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from nltk.tree import Tree\n",
    "from collections import *\n",
    "from gensim.models import word2vec\n",
    "import gensim, logging\n",
    "import tensorflow\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    " \n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "with open('training.json') as data_file:    \n",
    "    train_data = json.load(data_file)\n",
    "    \n",
    "with open('development.json') as data_file:    \n",
    "    dev_data = json.load(data_file)\n",
    "    \n",
    "with open('testing.json') as data_file:    \n",
    "    test_data = json.load(data_file)\n",
    "\n",
    "#pprint(data)\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NNP'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = (\n",
    "  'Pusheen and Smitha walked along the beach. '\n",
    "  'Pusheen wanted to surf, but fell off the surfboard.')\n",
    "output = nlp.annotate(text, properties={\n",
    "  'annotators': 'tokenize,ssplit,pos,depparse,parse',\n",
    "  'outputFormat': 'json'\n",
    "  })\n",
    "output['sentences'][0]['tokens'][0]['pos']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.14587439],\n",
       "       [ 0.14587439,  1.        ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(min_df=1)\n",
    "tfidf = vect.fit_transform(['Pusheen and Smitha walked along the beach. ',\n",
    "                           'Pusheen wanted to surf, but fell off the surfboard.'])\n",
    "(tfidf * tfidf.T).A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCorpus(data):\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for i in range(0, len(data['data'])):\n",
    "    #for i in range(0, 1):\n",
    "        dataset = data['data'][i]\n",
    "        for j in range(len(dataset['paragraphs'])):\n",
    "        #for j in range(0, 1):\n",
    "            question_bucket = dataset['paragraphs'][j]\n",
    "            contexts.append(question_bucket['context'])\n",
    "            for k in range(len(question_bucket['qas'])):\n",
    "                questions.append(question_bucket['qas'][k]['question'])\n",
    "                answers.append(question_bucket['qas'][k]['answers'][0]['text'])\n",
    "    return contexts, questions, answers\n",
    "\n",
    "train_contexts, train_questions, train_answers = getCorpus(train_data)\n",
    "dev_contexts, dev_questions, dev_answers = getCorpus(dev_data)\n",
    "test_contexts, test_questions, test_answers = getCorpus(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-04 19:31:35,441 : INFO : collecting all words and their counts\n",
      "2017-11-04 19:31:35,447 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-04 19:31:35,494 : INFO : PROGRESS: at sentence #10000, processed 166897 words, keeping 21853 word types\n",
      "2017-11-04 19:31:35,567 : INFO : PROGRESS: at sentence #20000, processed 336603 words, keeping 34412 word types\n",
      "2017-11-04 19:31:35,647 : INFO : PROGRESS: at sentence #30000, processed 521823 words, keeping 44726 word types\n",
      "2017-11-04 19:31:35,705 : INFO : PROGRESS: at sentence #40000, processed 704393 words, keeping 53541 word types\n",
      "2017-11-04 19:31:35,766 : INFO : PROGRESS: at sentence #50000, processed 889949 words, keeping 62130 word types\n",
      "2017-11-04 19:31:35,825 : INFO : PROGRESS: at sentence #60000, processed 1072611 words, keeping 69472 word types\n",
      "2017-11-04 19:31:35,887 : INFO : PROGRESS: at sentence #70000, processed 1260605 words, keeping 76791 word types\n",
      "2017-11-04 19:31:35,951 : INFO : PROGRESS: at sentence #80000, processed 1445498 words, keeping 84385 word types\n",
      "2017-11-04 19:31:36,015 : INFO : PROGRESS: at sentence #90000, processed 1623022 words, keeping 90142 word types\n",
      "2017-11-04 19:31:36,065 : INFO : PROGRESS: at sentence #100000, processed 1800001 words, keeping 95998 word types\n",
      "2017-11-04 19:31:36,130 : INFO : PROGRESS: at sentence #110000, processed 1980237 words, keeping 101793 word types\n",
      "2017-11-04 19:31:36,189 : INFO : PROGRESS: at sentence #120000, processed 2168599 words, keeping 107783 word types\n",
      "2017-11-04 19:31:36,253 : INFO : PROGRESS: at sentence #130000, processed 2347814 words, keeping 112949 word types\n",
      "2017-11-04 19:31:36,300 : INFO : PROGRESS: at sentence #140000, processed 2495858 words, keeping 116091 word types\n",
      "2017-11-04 19:31:36,340 : INFO : PROGRESS: at sentence #150000, processed 2600511 words, keeping 116709 word types\n",
      "2017-11-04 19:31:36,375 : INFO : PROGRESS: at sentence #160000, processed 2702946 words, keeping 117582 word types\n",
      "2017-11-04 19:31:36,410 : INFO : PROGRESS: at sentence #170000, processed 2805079 words, keeping 118325 word types\n",
      "2017-11-04 19:31:36,449 : INFO : PROGRESS: at sentence #180000, processed 2905884 words, keeping 118984 word types\n",
      "2017-11-04 19:31:36,487 : INFO : PROGRESS: at sentence #190000, processed 3001802 words, keeping 119703 word types\n",
      "2017-11-04 19:31:36,531 : INFO : PROGRESS: at sentence #200000, processed 3099004 words, keeping 120402 word types\n",
      "2017-11-04 19:31:36,565 : INFO : PROGRESS: at sentence #210000, processed 3198200 words, keeping 121101 word types\n",
      "2017-11-04 19:31:36,606 : INFO : PROGRESS: at sentence #220000, processed 3300529 words, keeping 121588 word types\n",
      "2017-11-04 19:31:36,665 : INFO : PROGRESS: at sentence #230000, processed 3400160 words, keeping 122216 word types\n",
      "2017-11-04 19:31:36,700 : INFO : PROGRESS: at sentence #240000, processed 3465528 words, keeping 122463 word types\n",
      "2017-11-04 19:31:36,723 : INFO : PROGRESS: at sentence #250000, processed 3491181 words, keeping 122529 word types\n",
      "2017-11-04 19:31:36,746 : INFO : PROGRESS: at sentence #260000, processed 3521502 words, keeping 122577 word types\n",
      "2017-11-04 19:31:36,769 : INFO : PROGRESS: at sentence #270000, processed 3554145 words, keeping 122620 word types\n",
      "2017-11-04 19:31:36,797 : INFO : PROGRESS: at sentence #280000, processed 3593727 words, keeping 122653 word types\n",
      "2017-11-04 19:31:36,822 : INFO : PROGRESS: at sentence #290000, processed 3625823 words, keeping 122699 word types\n",
      "2017-11-04 19:31:36,844 : INFO : PROGRESS: at sentence #300000, processed 3664705 words, keeping 122744 word types\n",
      "2017-11-04 19:31:36,869 : INFO : PROGRESS: at sentence #310000, processed 3696628 words, keeping 122812 word types\n",
      "2017-11-04 19:31:36,905 : INFO : PROGRESS: at sentence #320000, processed 3797678 words, keeping 122816 word types\n",
      "2017-11-04 19:31:36,919 : INFO : collected 122816 word types from a corpus of 3824585 raw words and 322571 sentences\n",
      "2017-11-04 19:31:36,920 : INFO : Loading a fresh vocabulary\n",
      "2017-11-04 19:31:37,413 : INFO : min_count=1 retains 122816 unique words (100% of original 122816, drops 0)\n",
      "2017-11-04 19:31:37,415 : INFO : min_count=1 leaves 3824585 word corpus (100% of original 3824585, drops 0)\n",
      "2017-11-04 19:31:37,924 : INFO : deleting the raw counts dictionary of 122816 items\n",
      "2017-11-04 19:31:37,928 : INFO : sample=0.001 downsamples 31 most-common words\n",
      "2017-11-04 19:31:37,929 : INFO : downsampling leaves estimated 3035581 word corpus (79.4% of prior 3824585)\n",
      "2017-11-04 19:31:37,931 : INFO : estimated required memory for 122816 words and 200 dimensions: 257913600 bytes\n",
      "2017-11-04 19:31:38,471 : INFO : resetting layer weights\n",
      "2017-11-04 19:31:41,040 : INFO : training model with 4 workers on 122816 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-11-04 19:31:42,138 : INFO : PROGRESS: at 2.39% examples, 536259 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-04 19:31:43,143 : INFO : PROGRESS: at 5.05% examples, 580449 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-04 19:31:44,175 : INFO : PROGRESS: at 7.11% examples, 540036 words/s, in_qsize 6, out_qsize 1\n",
      "2017-11-04 19:31:45,179 : INFO : PROGRESS: at 10.67% examples, 553649 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-04 19:31:46,183 : INFO : PROGRESS: at 15.82% examples, 548585 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-04 19:31:47,199 : INFO : PROGRESS: at 21.39% examples, 549987 words/s, in_qsize 6, out_qsize 1\n",
      "2017-11-04 19:31:48,204 : INFO : PROGRESS: at 23.81% examples, 552887 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-04 19:31:49,214 : INFO : PROGRESS: at 26.33% examples, 556405 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-04 19:31:50,222 : INFO : PROGRESS: at 29.28% examples, 560921 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-04 19:31:51,246 : INFO : PROGRESS: at 34.79% examples, 569926 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-04 19:31:52,249 : INFO : PROGRESS: at 41.60% examples, 577629 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-04 19:31:53,250 : INFO : PROGRESS: at 44.41% examples, 584951 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-04 19:31:54,258 : INFO : PROGRESS: at 47.86% examples, 601150 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-04 19:31:55,259 : INFO : PROGRESS: at 53.02% examples, 608561 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-04 19:31:56,268 : INFO : PROGRESS: at 60.82% examples, 613262 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-04 19:31:57,270 : INFO : PROGRESS: at 63.91% examples, 620039 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-04 19:31:58,275 : INFO : PROGRESS: at 67.28% examples, 629558 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-04 19:31:59,279 : INFO : PROGRESS: at 71.38% examples, 630071 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-04 19:32:00,295 : INFO : PROGRESS: at 79.08% examples, 627986 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-04 19:32:01,302 : INFO : PROGRESS: at 82.30% examples, 627729 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-04 19:32:02,306 : INFO : PROGRESS: at 85.61% examples, 634719 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-04 19:32:03,319 : INFO : PROGRESS: at 88.94% examples, 638284 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-04 19:32:04,328 : INFO : PROGRESS: at 94.08% examples, 638154 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-04 19:32:04,898 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-11-04 19:32:04,904 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-04 19:32:04,911 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-04 19:32:04,912 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-04 19:32:04,913 : INFO : training on 19122925 raw words (15177181 effective words) took 23.8s, 638140 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# import modules & set up logging\n",
    "import gensim, logging\n",
    "import string\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def getVectorizedCorpusModel(contexts, questions, answers):\n",
    "    contexts_filtered = [c.split('.') for c in contexts]\n",
    "    contexts_out = [d.translate(translator).split() for c in contexts_filtered for d in c]\n",
    "    questions_out = [c.translate(translator).split() for c in questions]\n",
    "    answers_out = [c.translate(translator).split() for c in answers]\n",
    "    out = contexts_out + questions_out + answers_out\n",
    "    return gensim.models.Word2Vec(out, size=200, window=5, min_count=1, workers=4)\n",
    "\n",
    "model = getVectorizedCorpusModel(train_contexts + dev_contexts + test_contexts,\n",
    "                                 train_questions + dev_questions + test_questions,\n",
    "                                 train_answers + test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = None\n",
    "for t in train_contexts:\n",
    "    output = nlp.annotate(t, properties={\n",
    "      'annotators': 'pos,ner',\n",
    "      'outputFormat': 'json'\n",
    "      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IN', 'VBN', 'RB', 'VB', '.', 'MD', 'PRP$', 'NNS', 'CC', ',', 'NNP', 'VBP', 'JJ', 'VBZ', 'TO', 'NN', 'DT', ':', 'PRP'}\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOCATION', 'O', 'LOCATION', 'O', 'LOCATION', 'LOCATION', 'O', 'LOCATION', 'O', 'LOCATION', 'O', 'LOCATION', 'O', 'LOCATION', 'LOCATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PERSON', 'PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOCATION', 'O']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.021739130434782608,\n",
       " 0.10869565217391304,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.8695652173913043]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### filter out non-content words\n",
    "def remove_stop_words(sentence):\n",
    "    result = ''\n",
    "    for t in sentence.split():\n",
    "        if t.lower() not in stop_words:\n",
    "            result += (t + ' ')\n",
    "    return result.rstrip()\n",
    "\n",
    "def calculateSimilarity(a, b):\n",
    "    context_sentences = [a, b]\n",
    "    tfidf = vect.fit_transform(context_sentences)\n",
    "    prod = (tfidf * tfidf.T).A\n",
    "    return prod[0][1]\n",
    "    \n",
    "def getPosNerList(sentence):\n",
    "    output = nlp.annotate(sentence, properties={\n",
    "      'annotators': 'pos,ner',\n",
    "      'outputFormat': 'json'\n",
    "      })\n",
    "    posList = []\n",
    "    nerList = []\n",
    "    for s in output['sentences']:\n",
    "        for t in s['tokens']:\n",
    "            posList.append(t['pos'])\n",
    "            nerList.append(t['ner'])\n",
    "    return posList,nerList\n",
    "            \n",
    "def getNerFracVector(annotateList):\n",
    "    nerType = {'PERSON': 0, 'LOCATION': 1, 'ORGANIZATION': 2, 'DATE': 3, 'TIME': 4, 'MONEY': 5, 'PERCENT': 6, 'MISC': 7, 'O': 8}\n",
    "    freq = [0] * len(nerType)\n",
    "    for a in annotateList:\n",
    "        if a in nerType:\n",
    "            freq[nerType[a]] += 1\n",
    "    \n",
    "    if len(annotateList) > 0:\n",
    "        for i in range(len(freq)):\n",
    "            freq[i] /= (1.0 * len(annotateList))\n",
    "    return freq\n",
    "    \n",
    "def getPosFracVector(annotateList):\n",
    "    posType = {'NNP': 0, 'NNPS': 1, 'NN': 2, 'NNS': 3}\n",
    "    freq = [0] * len(posType)\n",
    "    for a in annotateList:\n",
    "        if a in posType:\n",
    "            freq[posType[a]] += 1\n",
    "    \n",
    "    if len(annotateList) > 0:\n",
    "        for i in range(len(freq)):\n",
    "            freq[i] /= (1.0 * len(annotateList))\n",
    "    return freq\n",
    "    \n",
    "def getQuestionTypeVector(question):\n",
    "    qType = {'what': 0, 'where': 1, 'which': 2, 'who': 3, 'when': 4, 'why': 5, 'how': 6}\n",
    "    qWord = 'what'\n",
    "    for d in question:\n",
    "        if d.lower() in qType:\n",
    "            qWord = d.lower()\n",
    "    vec = [0] * 7\n",
    "    vec[qType[qWord]] = 1\n",
    "    return vec\n",
    "\n",
    "def getFeatureForQuestionAndContext(question, q_pos, q_ner, context, c_pos, c_ner):\n",
    "    q = question.translate(translator).split()\n",
    "    c = context.translate(translator).split()\n",
    "    \n",
    "    feat = sum(model[k] for k in q) / len(q)\n",
    "    if len(c) > 0:\n",
    "        feat += sum(model[k] for k in c) / len(c)\n",
    "    \n",
    "    # Use word embeddings for current question\n",
    "    #feat = (sum(model[k] for k in q) / len(q) + sum(model[k] for k in c) / len(c)).tolist()\n",
    "    feat = feat.tolist()\n",
    "    \n",
    "    # Add similarity bewteen question and current sentence\n",
    "    feat.append(calculateSimilarity(question, context))\n",
    "\n",
    "    # Add question Type\n",
    "    feat += getQuestionTypeVector(q)\n",
    "\n",
    "    # Add pos and ner overlap between question and context sentence\n",
    "    #q_pos, q_ner = getPosNerList(remove_stop_words(question))\n",
    "    #c_pos, c_ner = getPosNerList(remove_stop_words(context))\n",
    "\n",
    "    q_pos_frac = getPosFracVector(q_pos)\n",
    "    c_pos_frac = getPosFracVector(c_pos)\n",
    "    posOverlap = [0] * len(q_pos_frac)\n",
    "    for i in range(len(posOverlap)):\n",
    "        posOverlap[i] = q_pos_frac[i] * c_pos_frac[i]\n",
    "\n",
    "    feat += posOverlap\n",
    "\n",
    "    q_ner_frac = getNerFracVector(q_ner)\n",
    "    c_ner_frac = getNerFracVector(c_ner)\n",
    "    nerOverlap = [0] * len(q_ner_frac)\n",
    "    for i in range(len(nerOverlap)):\n",
    "        nerOverlap[i] = q_ner_frac[i] * c_ner_frac[i]\n",
    "\n",
    "    feat += nerOverlap\n",
    "\n",
    "    '''print(question)\n",
    "    print(q_pos)\n",
    "    print(q_pos_frac)\n",
    "    print(q_ner)\n",
    "    print(q_ner_frac)\n",
    "\n",
    "\n",
    "    print(context)\n",
    "    print(c_pos)\n",
    "    print(c_pos_frac)\n",
    "    print(c_ner)\n",
    "    print(c_ner_frac)\n",
    "\n",
    "    print(posOverlap)\n",
    "    print(nerOverlap)\n",
    "\n",
    "    print(len(q))\n",
    "    print(feat)'''\n",
    "    #return np.array(feat)\n",
    "    return feat\n",
    "\n",
    "#contexts = [d.strip() for d in data['data'][0]['paragraphs'][0]['context'].split('.')]\n",
    "#question = data['data'][0]['paragraphs'][0]['qas'][0]['question']\n",
    "#X = getFeatureForQuestionAndContext(question, contexts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0] * len(contexts)\n",
    "ans = data['data'][0]['paragraphs'][0]['qas'][4]['answers'][0]['text']\n",
    "for i in range(len(y)):\n",
    "    if ans in contexts[i]:\n",
    "        y[i] = 1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#contexts = [d.strip() for d in data['data'][0]['paragraphs'][0]['context'].split('.')]\n",
    "#question = data['data'][0]['paragraphs'][0]['qas'][0]['question']\n",
    "#X = getFeatureForQuestionAndContext(question, contexts[0])\n",
    "\n",
    "def getFeatureAndLabelForParagraph(paragraph):\n",
    "    contexts = [d.strip() for d in paragraph['context'].split('.') if len(d) > 0]\n",
    "    X = []\n",
    "    y = []\n",
    "    c_pos = []\n",
    "    c_ner = []\n",
    "    for i in range(len(contexts)):\n",
    "        c_pos_i, c_ner_i = getPosNerList(remove_stop_words(contexts[i]))\n",
    "        c_pos.append(c_pos_i)\n",
    "        c_ner.append(c_ner_i)\n",
    "        \n",
    "    for qas in paragraph['qas']:\n",
    "        question = qas['question']\n",
    "        answer = qas['answers'][0]['text']\n",
    "        q_pos, q_ner = getPosNerList(remove_stop_words(question))\n",
    "\n",
    "        for i in range(len(contexts)):\n",
    "            if len(contexts[i]) > 0 and len(question) > 0:\n",
    "                X.append(getFeatureForQuestionAndContext(question, q_pos, q_ner, contexts[i], c_pos[i], c_ner[i]))\n",
    "                y.append(1 if answer in contexts[i] else 0)\n",
    "    return X, y, len(contexts)\n",
    "\n",
    "def getFeatureAndLabel(data):\n",
    "    X = []\n",
    "    y = []\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(0, len(data['data'])):\n",
    "        if counter > 500:\n",
    "            break\n",
    "    #for i in range(0, 1):\n",
    "        dataset = data['data'][i]\n",
    "        for j in range(len(dataset['paragraphs'])):\n",
    "        #for j in range(0, 1):\n",
    "            counter += 1\n",
    "            if counter % 500 == 0:\n",
    "                print(counter)\n",
    "            question_bucket = dataset['paragraphs'][j]\n",
    "            Xj, yj, _ = getFeatureAndLabelForParagraph(question_bucket)\n",
    "            X += Xj\n",
    "            y += yj\n",
    "    return X, y\n",
    "#X0, y0 = getFeatureAndLabelForParagraph(data['data'][0]['paragraphs'][0])\n",
    "#X1, y1 = getFeatureAndLabelForParagraph(data['data'][0]['paragraphs'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "X, y = getFeatureAndLabel(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeatureForParagraph(paragraph):\n",
    "    contexts = [d.strip() for d in paragraph['context'].split('.') if len(d) > 0]\n",
    "    X = []\n",
    "    c_pos = []\n",
    "    c_ner = []\n",
    "    for i in range(len(contexts)):\n",
    "        c_pos_i, c_ner_i = getPosNerList(remove_stop_words(contexts[i]))\n",
    "        c_pos.append(c_pos_i)\n",
    "        c_ner.append(c_ner_i)\n",
    "        \n",
    "    for qas in paragraph['qas']:\n",
    "        question = qas['question']\n",
    "        q_pos, q_ner = getPosNerList(remove_stop_words(question))\n",
    "\n",
    "        for i in range(len(contexts)):\n",
    "            if len(contexts[i]) > 0 and len(question) > 0:\n",
    "                X.append(getFeatureForQuestionAndContext(question, q_pos, q_ner, contexts[i], c_pos[i], c_ner[i]))\n",
    "    return X, len(contexts)\n",
    "\n",
    "def getFeature(data):\n",
    "    X = []\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(0, len(data['data'])):\n",
    "    #for i in range(0, 1):\n",
    "        dataset = data['data'][i]\n",
    "        for j in range(len(dataset['paragraphs'])):\n",
    "        #for j in range(0, 1):\n",
    "            counter += 1\n",
    "            if counter % 100 == 0:\n",
    "                print(counter)\n",
    "            question_bucket = dataset['paragraphs'][j]\n",
    "            Xj, context_size = getFeatureForParagraph(question_bucket)\n",
    "            X += Xj\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9596"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(10, 10), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "#clf = linear_model.SGDClassifier()\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(10, 10), random_state=1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0]\n",
      "[0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1\n",
      " 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0\n",
      " 1]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "testX, testY, context_size = getFeatureAndLabelForParagraph(dev_data['data'][0]['paragraphs'][2])\n",
    "print(clf.predict(testX))\n",
    "result = clf.predict(testX)\n",
    "print(np.array(testY))\n",
    "print(context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 3)"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = np.resize(result, (int(len(result) / context_size), context_size))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_answer_for_file(file_name, model):\n",
    "    with open(file_name) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "    our_pred = {}\n",
    "    for i in range(0, len(data['data'])):\n",
    "    #for i in range(0, 1):\n",
    "        dataset = data['data'][i]\n",
    "        for j in range(len(dataset['paragraphs'])):\n",
    "        #for j in range(0, 1):\n",
    "            question_bucket = dataset['paragraphs'][j]\n",
    "            X_prime, context_size = getFeatureForParagraph(question_bucket)\n",
    "            y_pred = model.predict(X_prime)\n",
    "            y_pred = np.resize(y_pred, (int(len(y_pred) / context_size), context_size))\n",
    "            contexts = [d.strip() for d in question_bucket['context'].split('.') if len(d) > 0]\n",
    "\n",
    "            for k in range(len(question_bucket['qas'])):\n",
    "                question = question_bucket['qas'][k]['question']\n",
    "                qid = question_bucket['qas'][k]['id']\n",
    "                #our_pred[qid] = answer\n",
    "                candIndex = -1\n",
    "                if k >= y_pred.shape[0]:\n",
    "                    candIndex = random.choice(range(context_size))\n",
    "                else:\n",
    "                    for l in range(len(y_pred[k])):\n",
    "                        if y_pred[k][l] == 1:\n",
    "                            candIndex = l\n",
    "                            break;\n",
    "                    if candIndex == -1:\n",
    "                        candIndex = random.choice(range(context_size))\n",
    "                    \n",
    "                our_pred[qid] = contexts[candIndex]\n",
    "\n",
    "    return our_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "our_pred = generate_answer_for_file('development.json', clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pred_development.json', 'w') as outfile:\n",
    "    json.dump(our_pred, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'A pub /pʌb/, or public house is, despite its name, a private house, but is called a public house because it is licensed to sell alcohol to the general public. It is a drinking establishment in Britain, Ireland, New Zealand, Australia, Canada, Denmark and New England. In many places, especially in villages, a pub can be the focal point of the community. The writings of Samuel Pepys describe the pub as the heart of England.',\n",
       " 'qas': [{'answers': [{'answer_start': 105,\n",
       "     'text': 'it is licensed to sell alcohol'}],\n",
       "   'id': '56dede3c3277331400b4d784',\n",
       "   'question': 'What is a pub licensed to sell?'},\n",
       "  {'answers': [{'answer_start': 393, 'text': 'the pub'}],\n",
       "   'id': '56dede3c3277331400b4d785',\n",
       "   'question': 'In many villages what establishment could be called the focal point of the community?'},\n",
       "  {'answers': [{'answer_start': 16, 'text': 'public house'}],\n",
       "   'id': '56dfb4987aa994140058e003',\n",
       "   'question': \"What is the term 'pub' short for?\"},\n",
       "  {'answers': [{'answer_start': 255, 'text': 'New England'}],\n",
       "   'id': '56dfb4987aa994140058e004',\n",
       "   'question': 'Where in the United States are pubs located?'},\n",
       "  {'answers': [{'answer_start': 243, 'text': 'Denmark'}],\n",
       "   'id': '56dfb4987aa994140058e005',\n",
       "   'question': 'What continental European country has pubs?'},\n",
       "  {'answers': [{'answer_start': 235, 'text': 'Canada'}],\n",
       "   'id': '56dfb4987aa994140058e006',\n",
       "   'question': 'Other than the United States, where in North America are pubs located?'},\n",
       "  {'answers': [{'answer_start': 371, 'text': 'Samuel Pepys'}],\n",
       "   'id': '56dfb4987aa994140058e007',\n",
       "   'question': 'Who said that pubs are the heart of England?'}]}"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'][0]['paragraphs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00145477,  0.00253437,  0.00426356,  0.0032987 , -0.00381742,\n",
       "       -0.00113026,  0.00283679,  0.00299052, -0.0005114 ,  0.0009465 ,\n",
       "        0.00310909,  0.00136164,  0.00442152,  0.00102174, -0.00294037,\n",
       "        0.00392738, -0.0022461 ,  0.00126364,  0.00200181, -0.00328614,\n",
       "       -0.00325674,  0.00141178, -0.00187451,  0.00371888, -0.0019388 ,\n",
       "        0.00343885,  0.00489268,  0.00273538,  0.00060415, -0.00419724,\n",
       "        0.00175545, -0.0012204 ,  0.0010134 , -0.00355457, -0.00191119,\n",
       "       -0.00049069, -0.00327332,  0.00034621, -0.00088512,  0.000737  ,\n",
       "       -0.00431812,  0.00413071, -0.00044786,  0.00357864,  0.00268949,\n",
       "       -0.00320058, -0.0028346 , -0.00068395,  0.0026268 , -0.00312221,\n",
       "        0.00366472,  0.00118429,  0.00095956,  0.00397674, -0.00308599,\n",
       "        0.00235966, -0.00461775,  0.00357801,  0.00182779, -0.00073156,\n",
       "        0.00489754, -0.00193946, -0.00405051, -0.00223823,  0.00201041,\n",
       "       -0.00232397,  0.00427489, -0.00122443, -0.00175184, -0.0048895 ,\n",
       "        0.00394674,  0.00216936,  0.00207638, -0.00273139, -0.00273956,\n",
       "        0.00408027,  0.00478814,  0.00361099, -0.00358504,  0.00027186,\n",
       "       -0.00351077, -0.00229507, -0.00058858,  0.00070206,  0.0019421 ,\n",
       "        0.00148512, -0.0039563 ,  0.0045766 ,  0.00358422, -0.00384177,\n",
       "        0.00433829, -0.00086346,  0.00049587, -0.00495072, -0.00066207,\n",
       "       -0.00083211, -0.0038868 , -0.00069126, -0.00260562,  0.0010382 ], dtype=float32)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['pub']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A pub /pʌb/, or public house is, despite its name, a private house, but is called a public house because it is licensed to sell alcohol to the general public', 'It is a drinking establishment in Britain, Ireland, New Zealand, Australia, Canada, Denmark and New England', 'In many places, especially in villages, a pub can be the focal point of the community', 'The writings of Samuel Pepys describe the pub as the heart of England', 'Where in the United States are pubs located?']\n",
      "[[ 1.          0.10203864  0.04592331  0.06078796  0.01876728]\n",
      " [ 0.10203864  1.          0.06300119  0.04680124  0.0439374 ]\n",
      " [ 0.04592331  0.06300119  1.          0.28424234  0.16231839]\n",
      " [ 0.06078796  0.04680124  0.28424234  1.          0.10329405]\n",
      " [ 0.01876728  0.0439374   0.16231839  0.10329405  1.        ]]\n",
      "In many places, especially in villages, a pub can be the focal point of the community\n",
      "(ROOT\n",
      "  (SBARQ\n",
      "    (WHADVP (WRB Where)\n",
      "      (PP (IN in)\n",
      "        (NP (DT the) (NNP United) (NNPS States))))\n",
      "    (SQ (VBP are)\n",
      "      (NP (NNS pubs))\n",
      "      (ADJP (JJ located)))\n",
      "    (. ?)))\n",
      "the United States\n",
      "pubs\n",
      "(ROOT\n",
      "  (S\n",
      "    (PP (IN In)\n",
      "      (NP (JJ many) (NNS places)))\n",
      "    (, ,)\n",
      "    (PP\n",
      "      (ADVP (RB especially))\n",
      "      (IN in)\n",
      "      (NP (NNS villages)))\n",
      "    (, ,)\n",
      "    (NP (DT a) (NN pub))\n",
      "    (VP (MD can)\n",
      "      (VP (VB be)\n",
      "        (NP\n",
      "          (NP (DT the) (JJ focal) (NN point))\n",
      "          (PP (IN of)\n",
      "            (NP (DT the) (NN community))))))))\n",
      "many places\n",
      "villages\n",
      "a pub\n",
      "the focal point of the community\n",
      "the focal point\n",
      "the community\n"
     ]
    }
   ],
   "source": [
    "context = data['data'][0]['paragraphs'][0]['context']\n",
    "question = data['data'][0]['paragraphs'][0]['qas'][3]['question']\n",
    "context_sentences = [c.lstrip() for c in context.split('.') if len(c) > 0]\n",
    "context_sentences.append(question)\n",
    "print(context_sentences)\n",
    "tfidf = vect.fit_transform(context_sentences)\n",
    "prod = (tfidf * tfidf.T).A\n",
    "size = prod.shape[0]\n",
    "print(prod)\n",
    "condidate_sentence_index = np.argmax(prod[size - 1][:size - 1])\n",
    "print(context_sentences[condidate_sentence_index])\n",
    "\n",
    "output = nlp.annotate(question, properties={\n",
    "  'annotators': 'pos,ner,parse',\n",
    "  'outputFormat': 'json'\n",
    "  })\n",
    "print(output['sentences'][0]['parse'])\n",
    "parsestr = output['sentences'][0]['parse']\n",
    "for i in Tree.fromstring(parsestr).subtrees():\n",
    "    if i.label() == 'NP':\n",
    "        print(' '.join(i.leaves()))\n",
    "\n",
    "output = nlp.annotate(context_sentences[condidate_sentence_index], properties={\n",
    "  'annotators': 'pos,ner,parse',\n",
    "  'outputFormat': 'json'\n",
    "  })\n",
    "print(output['sentences'][0]['parse'])\n",
    "parsestr = output['sentences'][0]['parse']\n",
    "for i in Tree.fromstring(parsestr).subtrees():\n",
    "    if i.label() == 'NP':\n",
    "        print(' '.join(i.leaves()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0, 'licensed': 1, 'and': 2, 'new': 3, 'places,': 4, 'private': 5, 'focal': 6, 'but': 7, 'drinking': 8, 'heart': 9, 'england': 10, 'sell': 11, 'is,': 12, 'called': 13, 'despite': 14, 'in': 15, 'many': 16, 'describe': 17, 'name,': 18, 'point': 19, 'britain,': 20, 'canada,': 21, 'australia,': 22, 'especially': 23, 'or': 24, 'because': 25, 'pub': 26, 'to': 27, 'writings': 28, 'its': 29, 'community': 30, 'a': 31, 'public': 32, 'house,': 33, 'ireland,': 34, 'general': 35, 'it': 36, 'alcohol': 37, 'establishment': 38, 'denmark': 39, 'pepys': 40, 'samuel': 41, 'villages,': 42, 'can': 43, 'zealand,': 44, '/pʌb/,': 45, 'of': 46, 'as': 47, 'be': 48, 'house': 49, 'is': 50, 'the': 51}\n",
      "['', 'licensed', 'and', 'new', 'places,', 'private', 'focal', 'but', 'drinking', 'heart', 'england', 'sell', 'is,', 'called', 'despite', 'in', 'many', 'describe', 'name,', 'point', 'britain,', 'canada,', 'australia,', 'especially', 'or', 'because', 'pub', 'to', 'writings', 'its', 'community', 'a', 'public', 'house,', 'ireland,', 'general', 'it', 'alcohol', 'establishment', 'denmark', 'pepys', 'samuel', 'villages,', 'can', 'zealand,', '/pʌb/,', 'of', 'as', 'be', 'house', 'is', 'the']\n"
     ]
    }
   ],
   "source": [
    "# import modules & set up logging\n",
    "print(word2idx)\n",
    "print(idx2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.087885368218185836"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['Denmark'], negative=[], topn=3)\n",
    "#sum(model['England'])\n",
    "model.similarity('Denmark', 'England')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP (JJ many) (NNS places))\n",
      "(NP (NNS villages))\n",
      "(NP (DT a) (NN pub))\n",
      "(NP\n",
      "  (NP (DT the) (JJ focal) (NN point))\n",
      "  (PP (IN of) (NP (DT the) (NN community))))\n",
      "(NP (DT the) (JJ focal) (NN point))\n",
      "(NP (DT the) (NN community))\n"
     ]
    }
   ],
   "source": [
    "parsestr = output['sentences'][0]['parse']\n",
    "for i in Tree.fromstring(parsestr).subtrees():\n",
    "    if i.label() == 'NP':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text = ('Alice lives in England in January 2017')\n",
    "text = ('While in Cornell University in January 2017, at 1:30 AM, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.')\n",
    "output = nlp.annotate(text, properties={\n",
    "  'annotators': 'pos,ner,parse',\n",
    "  'outputFormat': 'json'\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'after': ' ',\n",
       "  'before': '',\n",
       "  'characterOffsetBegin': 0,\n",
       "  'characterOffsetEnd': 5,\n",
       "  'index': 1,\n",
       "  'lemma': 'while',\n",
       "  'ner': 'O',\n",
       "  'originalText': 'While',\n",
       "  'pos': 'IN',\n",
       "  'word': 'While'},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 6,\n",
       "  'characterOffsetEnd': 8,\n",
       "  'index': 2,\n",
       "  'lemma': 'in',\n",
       "  'ner': 'O',\n",
       "  'originalText': 'in',\n",
       "  'pos': 'IN',\n",
       "  'word': 'in'},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 9,\n",
       "  'characterOffsetEnd': 16,\n",
       "  'index': 3,\n",
       "  'lemma': 'Cornell',\n",
       "  'ner': 'ORGANIZATION',\n",
       "  'originalText': 'Cornell',\n",
       "  'pos': 'NNP',\n",
       "  'word': 'Cornell'},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 17,\n",
       "  'characterOffsetEnd': 27,\n",
       "  'index': 4,\n",
       "  'lemma': 'University',\n",
       "  'ner': 'ORGANIZATION',\n",
       "  'originalText': 'University',\n",
       "  'pos': 'NNP',\n",
       "  'word': 'University'},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 28,\n",
       "  'characterOffsetEnd': 30,\n",
       "  'index': 5,\n",
       "  'lemma': 'in',\n",
       "  'ner': 'O',\n",
       "  'originalText': 'in',\n",
       "  'pos': 'IN',\n",
       "  'word': 'in'},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 31,\n",
       "  'characterOffsetEnd': 38,\n",
       "  'index': 6,\n",
       "  'lemma': 'January',\n",
       "  'ner': 'DATE',\n",
       "  'normalizedNER': '2017-01',\n",
       "  'originalText': 'January',\n",
       "  'pos': 'NNP',\n",
       "  'timex': {'tid': 't1', 'type': 'DATE', 'value': '2017-01'},\n",
       "  'word': 'January'},\n",
       " {'after': '',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 39,\n",
       "  'characterOffsetEnd': 43,\n",
       "  'index': 7,\n",
       "  'lemma': '2017',\n",
       "  'ner': 'DATE',\n",
       "  'normalizedNER': '2017-01',\n",
       "  'originalText': '2017',\n",
       "  'pos': 'CD',\n",
       "  'timex': {'tid': 't1', 'type': 'DATE', 'value': '2017-01'},\n",
       "  'word': '2017'},\n",
       " {'after': ' ',\n",
       "  'before': '',\n",
       "  'characterOffsetBegin': 43,\n",
       "  'characterOffsetEnd': 44,\n",
       "  'index': 8,\n",
       "  'lemma': ',',\n",
       "  'ner': 'O',\n",
       "  'originalText': ',',\n",
       "  'pos': ',',\n",
       "  'word': ','},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 45,\n",
       "  'characterOffsetEnd': 47,\n",
       "  'index': 9,\n",
       "  'lemma': 'at',\n",
       "  'ner': 'O',\n",
       "  'originalText': 'at',\n",
       "  'pos': 'IN',\n",
       "  'word': 'at'},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 48,\n",
       "  'characterOffsetEnd': 52,\n",
       "  'index': 10,\n",
       "  'lemma': '1:30',\n",
       "  'ner': 'TIME',\n",
       "  'normalizedNER': 'T01:30',\n",
       "  'originalText': '1:30',\n",
       "  'pos': 'CD',\n",
       "  'timex': {'tid': 't2', 'type': 'TIME', 'value': 'T01:30'},\n",
       "  'word': '1:30'},\n",
       " {'after': '',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 53,\n",
       "  'characterOffsetEnd': 55,\n",
       "  'index': 11,\n",
       "  'lemma': 'AM',\n",
       "  'ner': 'TIME',\n",
       "  'normalizedNER': 'T01:30',\n",
       "  'originalText': 'AM',\n",
       "  'pos': 'NNP',\n",
       "  'timex': {'tid': 't2', 'type': 'TIME', 'value': 'T01:30'},\n",
       "  'word': 'AM'},\n",
       " {'after': ' ',\n",
       "  'before': '',\n",
       "  'characterOffsetBegin': 55,\n",
       "  'characterOffsetEnd': 56,\n",
       "  'index': 12,\n",
       "  'lemma': ',',\n",
       "  'ner': 'O',\n",
       "  'originalText': ',',\n",
       "  'pos': ',',\n",
       "  'word': ','},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 57,\n",
       "  'characterOffsetEnd': 66,\n",
       "  'index': 13,\n",
       "  'lemma': 'Christine',\n",
       "  'ner': 'PERSON',\n",
       "  'originalText': 'Christine',\n",
       "  'pos': 'NNP',\n",
       "  'word': 'Christine'},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 67,\n",
       "  'characterOffsetEnd': 74,\n",
       "  'index': 14,\n",
       "  'lemma': 'Lagarde',\n",
       "  'ner': 'PERSON',\n",
       "  'originalText': 'Lagarde',\n",
       "  'pos': 'NNP',\n",
       "  'word': 'Lagarde'},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 75,\n",
       "  'characterOffsetEnd': 84,\n",
       "  'index': 15,\n",
       "  'lemma': 'discuss',\n",
       "  'ner': 'O',\n",
       "  'originalText': 'discussed',\n",
       "  'pos': 'VBD',\n",
       "  'word': 'discussed'},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 85,\n",
       "  'characterOffsetEnd': 95,\n",
       "  'index': 16,\n",
       "  'lemma': 'short-term',\n",
       "  'ner': 'O',\n",
       "  'originalText': 'short-term',\n",
       "  'pos': 'JJ',\n",
       "  'word': 'short-term'},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 96,\n",
       "  'characterOffsetEnd': 104,\n",
       "  'index': 17,\n",
       "  'lemma': 'stimulus',\n",
       "  'ner': 'O',\n",
       "  'originalText': 'stimulus',\n",
       "  'pos': 'NN',\n",
       "  'word': 'stimulus'},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 105,\n",
       "  'characterOffsetEnd': 112,\n",
       "  'index': 18,\n",
       "  'lemma': 'effort',\n",
       "  'ner': 'O',\n",
       "  'originalText': 'efforts',\n",
       "  'pos': 'NNS',\n",
       "  'word': 'efforts'},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 113,\n",
       "  'characterOffsetEnd': 115,\n",
       "  'index': 19,\n",
       "  'lemma': 'in',\n",
       "  'ner': 'O',\n",
       "  'originalText': 'in',\n",
       "  'pos': 'IN',\n",
       "  'word': 'in'},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 116,\n",
       "  'characterOffsetEnd': 117,\n",
       "  'index': 20,\n",
       "  'lemma': 'a',\n",
       "  'ner': 'O',\n",
       "  'originalText': 'a',\n",
       "  'pos': 'DT',\n",
       "  'word': 'a'},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 118,\n",
       "  'characterOffsetEnd': 124,\n",
       "  'index': 21,\n",
       "  'lemma': 'recent',\n",
       "  'ner': 'O',\n",
       "  'originalText': 'recent',\n",
       "  'pos': 'JJ',\n",
       "  'word': 'recent'},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 125,\n",
       "  'characterOffsetEnd': 134,\n",
       "  'index': 22,\n",
       "  'lemma': 'interview',\n",
       "  'ner': 'O',\n",
       "  'originalText': 'interview',\n",
       "  'pos': 'NN',\n",
       "  'word': 'interview'},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 135,\n",
       "  'characterOffsetEnd': 139,\n",
       "  'index': 23,\n",
       "  'lemma': 'with',\n",
       "  'ner': 'O',\n",
       "  'originalText': 'with',\n",
       "  'pos': 'IN',\n",
       "  'word': 'with'},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 140,\n",
       "  'characterOffsetEnd': 143,\n",
       "  'index': 24,\n",
       "  'lemma': 'the',\n",
       "  'ner': 'O',\n",
       "  'originalText': 'the',\n",
       "  'pos': 'DT',\n",
       "  'word': 'the'},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 144,\n",
       "  'characterOffsetEnd': 148,\n",
       "  'index': 25,\n",
       "  'lemma': 'Wall',\n",
       "  'ner': 'ORGANIZATION',\n",
       "  'originalText': 'Wall',\n",
       "  'pos': 'NNP',\n",
       "  'word': 'Wall'},\n",
       " {'after': ' ',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 149,\n",
       "  'characterOffsetEnd': 155,\n",
       "  'index': 26,\n",
       "  'lemma': 'Street',\n",
       "  'ner': 'ORGANIZATION',\n",
       "  'originalText': 'Street',\n",
       "  'pos': 'NNP',\n",
       "  'word': 'Street'},\n",
       " {'after': '',\n",
       "  'before': ' ',\n",
       "  'characterOffsetBegin': 156,\n",
       "  'characterOffsetEnd': 163,\n",
       "  'index': 27,\n",
       "  'lemma': 'Journal',\n",
       "  'ner': 'ORGANIZATION',\n",
       "  'originalText': 'Journal',\n",
       "  'pos': 'NNP',\n",
       "  'word': 'Journal'},\n",
       " {'after': '',\n",
       "  'before': '',\n",
       "  'characterOffsetBegin': 163,\n",
       "  'characterOffsetEnd': 164,\n",
       "  'index': 28,\n",
       "  'lemma': '.',\n",
       "  'ner': 'O',\n",
       "  'originalText': '.',\n",
       "  'pos': '.',\n",
       "  'word': '.'}]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['sentences'][0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'annotate',\n",
       " 'regex',\n",
       " 'semgrex',\n",
       " 'tokensregex']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_list = [func for func in dir(StanfordCoreNLP) if callable(getattr(StanfordCoreNLP, func))]\n",
    "method_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('While', 'O'), ('in', 'O'), ('Cornell', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('January', 'O'), ('2017', 'O'), (',', 'O'), ('Christine', 'PERSON'), ('Lagarde', 'PERSON'), ('discussed', 'O'), ('short-term', 'O'), ('stimulus', 'O'), ('efforts', 'O'), ('in', 'O'), ('a', 'O'), ('recent', 'O'), ('interview', 'O'), ('with', 'O'), ('the', 'O'), ('Wall', 'ORGANIZATION'), ('Street', 'ORGANIZATION'), ('Journal', 'ORGANIZATION'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "st = StanfordNERTagger('/Users/shuohuang/Downloads/stanford-tools/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "\t\t\t\t\t   '/Users/shuohuang/Downloads/stanford-tools/stanford-ner/stanford-ner.jar',\n",
    "\t\t\t\t\t   encoding='utf-8')\n",
    "\n",
    "text = 'While in Cornell University in January 2017, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.'\n",
    "\n",
    "tokenized_text = word_tokenize(text)\n",
    "classified_text = st.tag(tokenized_text)\n",
    "\n",
    "\n",
    "print(classified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.tag.stanford.StanfordNERTagger at 0x11ec87c50>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_type = {'what':'NN', 'who':'NNP'}\n",
    "\n",
    "### filter out non-content words\n",
    "def remove_stop_words(sentence):\n",
    "    result = ''\n",
    "    for t in sentence.split():\n",
    "        if t.lower() not in stop_words:\n",
    "            result += (t + ' ')\n",
    "    return result.rstrip()\n",
    "\n",
    "### get all the content words in a sentence\n",
    "def get_content_token_set(sentence):\n",
    "    token_set = set()\n",
    "    for pt in sentence.translate(translator).split():\n",
    "        if pt.lower() not in stop_words:\n",
    "            token_set.add(pt.lower())\n",
    "    return token_set\n",
    "\n",
    "### check the number of overlaps from context sentence tokens with question sentence tokens\n",
    "def compare(q_text_set, c_text_set):\n",
    "    score = 0\n",
    "    for c_token in c_text_set:\n",
    "        if c_token in q_text_set:\n",
    "            score += 1\n",
    "    return score\n",
    "\n",
    "### find the sentence from context corpus which has the most overlaps with the question\n",
    "def find_candidate_sentence(q_token_set, context):\n",
    "    candidate = context[0]\n",
    "    score = compare(q_token_set, get_content_token_set(candidate))\n",
    "    for i in range(1, len(context)):\n",
    "        temp_candi = context[i]\n",
    "        temp_score = compare(q_token_set, get_content_token_set(temp_candi))\n",
    "        if temp_score >= score:\n",
    "            score = temp_score\n",
    "            candidate = temp_candi\n",
    "    return candidate\n",
    "\n",
    "### define question type\n",
    "def find_question_type(question):\n",
    "    for q in question.split():\n",
    "        if q.lower() == 'what' or q.lower() == 'which':\n",
    "            return 'NN'\n",
    "        elif q.lower() == 'who' or q.lower() == 'where':\n",
    "            return 'NNP'\n",
    "\n",
    "### \n",
    "def find_possible_answer_with_distance(q_token_set, candidate_sentence, q_type):\n",
    "    token_positions = {}\n",
    "    cand_tokens = candidate_sentence.split()\n",
    "    for i in range(len(cand_tokens)):\n",
    "        if cand_tokens[i].lower() in q_token_set:\n",
    "            token_positions[cand_tokens[i]] = i\n",
    "    #print(token_positions)\n",
    "    c_tag = nltk.pos_tag(word_tokenize(candidate_sentence))\n",
    "    possible_answers = set()\n",
    "    for c in c_tag:\n",
    "        if c[1] == q_type and c[0].lower() not in q_token_set:\n",
    "            possible_answers.add(c[0])\n",
    "    \n",
    "    answer_possitions = {}\n",
    "    for i in range(len(cand_tokens)):\n",
    "        if cand_tokens[i] in possible_answers:\n",
    "            answer_possitions[cand_tokens[i]] = i\n",
    "    answer_score = {}\n",
    "    for ap in answer_possitions:\n",
    "        position = answer_possitions[ap]\n",
    "        score = 0\n",
    "        for tp in token_positions:\n",
    "            score += abs(position - token_positions[tp])\n",
    "        answer_score[ap] = score\n",
    "    answer_score = sorted(answer_score.items(), key=operator.itemgetter(1))\n",
    "    #print(possible_answers)\n",
    "    #print(answer_possitions)\n",
    "    #print(answer_score)\n",
    "    \n",
    "    answer = str()\n",
    "    for ans in answer_score:\n",
    "        answer += (ans[0] + ' ')\n",
    "    \n",
    "    return answer.rstrip()\n",
    "\n",
    "def find_answer(question, context):\n",
    "    q_tag = nltk.pos_tag(word_tokenize(question))\n",
    "    q_token_set = get_content_token_set(question)\n",
    "    q_type = find_question_type(question)\n",
    "    context_sentences = [c.lstrip().translate(translator) for c in context.split('.') if len(c) > 0]\n",
    "    candidate_sentence = find_candidate_sentence(q_token_set, context_sentences)\n",
    "    #c_tag = nltk.pos_tag(word_tokenize(candidate_sentence))\n",
    "    answer = find_possible_answer_with_distance(q_token_set, candidate_sentence, q_type)\n",
    "    #print(question)\n",
    "    #print(q_type)\n",
    "    #print(q_token_set)\n",
    "    #print(candidate_sentence)\n",
    "    #print(c_tag)\n",
    "    #print(answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_answer_for_file(file_name):\n",
    "    with open(file_name) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "    our_pred = {}\n",
    "    for i in range(0, len(data['data'])):\n",
    "    #for i in range(0, 1):\n",
    "        dataset = data['data'][i]\n",
    "        for j in range(len(dataset['paragraphs'])):\n",
    "        #for j in range(0, 1):\n",
    "            question_bucket = dataset['paragraphs'][j]\n",
    "            context = question_bucket['context']\n",
    "            for k in range(len(question_bucket['qas'])):\n",
    "                question = question_bucket['qas'][k]['question']\n",
    "                qid = question_bucket['qas'][k]['id']\n",
    "                answer = find_answer(question, context)\n",
    "                our_pred[qid] = answer\n",
    "    return our_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "our_pred = generate_answer_for_file('development.json')\n",
    "with open('pred_development.json', 'w') as outfile:\n",
    "    json.dump(our_pred, outfile)\n",
    "    \n",
    "\n",
    "our_pred = generate_answer_for_file('testing.json')\n",
    "with open('pred_testing.json', 'w') as outfile:\n",
    "    json.dump(our_pred, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import math\n",
    "import operator\n",
    "\n",
    "os.getenv('CLASSPATH')\n",
    "#os.environ['STANFORD_PARSER'] = '/Users/shuohuang/Downloads/stanford-postagger'\n",
    "#os.environ['STANFORD_MODELS'] = '/Users/shuohuang/Downloads/stanford-postagger'\n",
    "#CLASSPATH = '/Users/shuohuang/Downloads/stanford-postagger'\n",
    "#posTagger = StanfordPOSTagger('/english-bidirectional-distsim.tagger')\n",
    "#nerTagger = StanfordNERTagger('~/Downloads/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz')\n",
    "\n",
    "with open('training.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "#pprint(data)\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_type = {'what':'NN', 'who':'NNP'}\n",
    "\n",
    "### filter out non-content words\n",
    "def remove_stop_words(sentence):\n",
    "    result = ''\n",
    "    for t in sentence.split():\n",
    "        if t.lower() not in stop_words:\n",
    "            result += (t + ' ')\n",
    "    return result.rstrip()\n",
    "\n",
    "### get all the content words in a sentence\n",
    "def get_content_token_set(sentence):\n",
    "    token_set = set()\n",
    "    for pt in sentence.translate(translator).split():\n",
    "        if pt.lower() not in stop_words:\n",
    "            token_set.add(pt.lower())\n",
    "    return token_set\n",
    "\n",
    "### check the number of overlaps from context sentence tokens with question sentence tokens\n",
    "def compare(q_text_set, c_text_set):\n",
    "    score = 0\n",
    "    for c_token in c_text_set:\n",
    "        if c_token in q_text_set:\n",
    "            score += 1\n",
    "    return score\n",
    "\n",
    "### find the sentence from context corpus which has the most overlaps with the question\n",
    "def find_candidate_sentence(q_token_set, context):\n",
    "    candidate = context[0]\n",
    "    score = compare(q_token_set, get_content_token_set(candidate))\n",
    "    for i in range(1, len(context)):\n",
    "        temp_candi = context[i]\n",
    "        temp_score = compare(q_token_set, get_content_token_set(temp_candi))\n",
    "        if temp_score >= score:\n",
    "            score = temp_score\n",
    "            candidate = temp_candi\n",
    "    return candidate\n",
    "\n",
    "### define question type\n",
    "def find_question_type(question):\n",
    "    for q in question.split():\n",
    "        if q.lower() == 'what' or q.lower() == 'which':\n",
    "            return 'NN'\n",
    "        elif q.lower() == 'who' or q.lower() == 'where':\n",
    "            return 'NNP'\n",
    "\n",
    "### \n",
    "def find_possible_answer_with_distance(q_token_set, candidate_sentence, q_type):\n",
    "    token_positions = {}\n",
    "    cand_tokens = candidate_sentence.split()\n",
    "    for i in range(len(cand_tokens)):\n",
    "        if cand_tokens[i].lower() in q_token_set:\n",
    "            token_positions[cand_tokens[i]] = i\n",
    "    #print(token_positions)\n",
    "    c_tag = nltk.pos_tag(word_tokenize(candidate_sentence))\n",
    "    possible_answers = set()\n",
    "    for c in c_tag:\n",
    "        if c[1] == q_type and c[0].lower() not in q_token_set:\n",
    "            possible_answers.add(c[0])\n",
    "    \n",
    "    answer_possitions = {}\n",
    "    for i in range(len(cand_tokens)):\n",
    "        if cand_tokens[i] in possible_answers:\n",
    "            answer_possitions[cand_tokens[i]] = i\n",
    "    answer_score = {}\n",
    "    for ap in answer_possitions:\n",
    "        position = answer_possitions[ap]\n",
    "        score = 0\n",
    "        for tp in token_positions:\n",
    "            score += abs(position - token_positions[tp])\n",
    "        answer_score[ap] = score\n",
    "    answer_score = sorted(answer_score.items(), key=operator.itemgetter(1))\n",
    "    #print(possible_answers)\n",
    "    #print(answer_possitions)\n",
    "    #print(answer_score)\n",
    "    \n",
    "    answer = str()\n",
    "    for ans in answer_score:\n",
    "        answer += (ans[0] + ' ')\n",
    "    \n",
    "    return answer.rstrip()\n",
    "\n",
    "def find_answer(question, context):\n",
    "    q_tag = nltk.pos_tag(word_tokenize(question))\n",
    "    q_token_set = get_content_token_set(question)\n",
    "    q_type = find_question_type(question)\n",
    "    context_sentences = [c.lstrip().translate(translator) for c in context.split('.') if len(c) > 0]\n",
    "    candidate_sentence = find_candidate_sentence(q_token_set, context_sentences)\n",
    "    #c_tag = nltk.pos_tag(word_tokenize(candidate_sentence))\n",
    "    answer = find_possible_answer_with_distance(q_token_set, candidate_sentence, q_type)\n",
    "    #print(question)\n",
    "    #print(q_type)\n",
    "    #print(q_token_set)\n",
    "    #print(candidate_sentence)\n",
    "    #print(c_tag)\n",
    "    #print(answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_for_file(file_name):\n",
    "    with open(file_name) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "    our_pred = {}\n",
    "    for i in range(0, len(data['data'])):\n",
    "    #for i in range(0, 1):\n",
    "        dataset = data['data'][i]\n",
    "        for j in range(len(dataset['paragraphs'])):\n",
    "        #for j in range(0, 1):\n",
    "            question_bucket = dataset['paragraphs'][j]\n",
    "            context = question_bucket['context']\n",
    "            for k in range(len(question_bucket['qas'])):\n",
    "                question = question_bucket['qas'][k]['question']\n",
    "                qid = question_bucket['qas'][k]['id']\n",
    "                answer = find_answer(question, context)\n",
    "                our_pred[qid] = answer\n",
    "    return our_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_pred = generate_answer_for_file('development.json')\n",
    "with open('pred_development.json', 'w') as outfile:\n",
    "    json.dump(our_pred, outfile)\n",
    "    \n",
    "\n",
    "our_pred = generate_answer_for_file('testing.json')\n",
    "with open('pred_testing.json', 'w') as outfile:\n",
    "    json.dump(our_pred, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
